import os
import json
import numpy as np
from tqdm import tqdm
from pathlib import Path
from .preprocessing import Preprocessor
from sentence_transformers import SentenceTransformer
import torch
import logging

logger = logging.getLogger('uvicorn.error')

class EmbeddingProcessor:
    """
    Class Embedding:
        - Nh√∫ng vƒÉn b·∫£n t·ª´ file HTML.
        - L∆∞u embeddings v√†o th∆∞ m·ª•c.

    Args:
        - file_paths: Danh s√°ch ƒë∆∞·ªùng d·∫´n file HTML.
        - labels: Danh s√°ch nh√£n t∆∞∆°ng ·ª©ng.
        - output_dir: Th∆∞ m·ª•c l∆∞u embeddings.
        - model_name: T√™n m√¥ h√¨nh embedding.
        - get_text_lib: Th∆∞ vi·ªán ƒë·ªçc vƒÉn b·∫£n t·ª´ file HTML (newspaper, inscriptis).
        - type: storage folder 
    
    Output:
        - T·∫°o file embeddings .npy v√† metadata .json.
        - Tr·∫£ v·ªÅ th∆∞ m·ª•c l∆∞u tr·ªØ, s·ªë l∆∞·ª£ng file v√† t·ªïng dung l∆∞·ª£ng th∆∞ m·ª•c.
    """

    MODELS = {
        "SBERT": "keepitreal/vietnamese-sbert",
        "Bi-Encoder": "bkai-foundation-models/vietnamese-bi-encoder",
    }

    def __init__(self, file_paths, output_dir="embedding", model_name="SBERT", type="train", batch_size=8):
        self.file_paths = file_paths
        self.output_dir = Path(output_dir) / model_name
        self.type = type
        self.batch_size = batch_size
        # self.error_files = []
        self.preprocessor = Preprocessor()

        if model_name not in self.MODELS:
            raise ValueError(f"Model {model_name} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.debug(f"üîç Using device: {self.device}")
        
        self.encoder = SentenceTransformer(self.MODELS[model_name]).to(self.device)
        self.output_dir.mkdir(parents=True, exist_ok=True)



    def process_embedding(self):
        """
        Nh√∫ng embedding v√† x·ª≠ l√Ω d·ªØ li·ªáu.
        """
        
        descriptions, file_names = self.preprocessor.process_files(self.file_paths)

        # Nh√∫ng vƒÉn b·∫£n b·∫±ng batch
        embeddings = self.batch_encode(descriptions)

        # L∆∞u d·ªØ li·ªáu
        self.save_embedding(embeddings, file_names, self.type)
        
        # Tr·∫£ v·ªÅ th√¥ng tin th∆∞ m·ª•c l∆∞u tr·ªØ
        return self.get_storage_info()

    def batch_encode(self, texts):
        """Nh√∫ng vƒÉn b·∫£n theo batch ƒë·ªÉ ti·∫øt ki·ªám RAM."""
        batched_embeddings = []
        for i in tqdm(range(0, len(texts), self.batch_size), desc="Encoding batch"):
            batch = texts[i : i + self.batch_size]
            batch_embeddings = self.encoder.encode(batch)
            # batch_embeddings = batch_embeddings.cpu().numpy()
            batched_embeddings.append(batch_embeddings)
        return np.vstack(batched_embeddings)

    def save_embedding(self, embeddings, file_names, type):
        """L∆∞u t·∫≠p d·ªØ li·ªáu embeddings."""
        np.save(self.output_dir / f"X_{type}.npy", embeddings)

        split_info = {f"{type}": file_names}
        with open(self.output_dir / f"split_info_{type}.json", "w", encoding="utf-8") as f:
            json.dump(split_info, f, ensure_ascii=False, indent=4)

        logger.debug(f"‚úÖ Saved embeddings in {self.output_dir}")

    def get_storage_info(self):
        """Tr·∫£ v·ªÅ th∆∞ m·ª•c l∆∞u tr·ªØ, s·ªë l∆∞·ª£ng file ƒë√£ x·ª≠ l√Ω v√† t·ªïng dung l∆∞·ª£ng th∆∞ m·ª•c."""
        total_size = sum(f.stat().st_size for f in self.output_dir.glob("**/*") if f.is_file())
        num_files = len(list(self.output_dir.glob("**/*")))
        
        return {
            "storage_folder": str(self.output_dir),
            "num_files": num_files,
            "total_size_bytes": total_size
        }
